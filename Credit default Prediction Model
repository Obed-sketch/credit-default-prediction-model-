import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
from flask import Flask, request, jsonify



# Load dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"
column_names = [
    'checking_account', 'duration', 'credit_history', 'purpose', 'credit_amount',
    'savings', 'employment', 'installment_rate', 'personal_status', 'debtors',
    'residence', 'property', 'age', 'other_plans', 'housing', 'existing_credits',
    'job', 'dependents', 'telephone', 'foreign', 'class'
]
df = pd.read_csv(url, delim_whitespace=True, names=column_names)

# Convert target to binary (1: bad, 0: good)
df['target'] = df['class'].map({1: 0, 2: 1})
df.drop('class', axis=1, inplace=True)

# Split features and target
X = df.drop('target', axis=1)
y = df['target']


#Train-to-test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Categorical and numerical columns (indices)
categorical_cols = [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19]
numerical_cols = [1, 4, 7, 10, 12, 15, 17]

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])


#Model Training with Cross-Validation and Hyperparameter Tuning
# Logistic Regression Pipeline
pipeline_lr = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', LogisticRegression())
])

param_grid_lr = {
    'classifier__C': [0.1, 1, 10],
    'classifier__solver': ['liblinear']
}

grid_lr = GridSearchCV(
    pipeline_lr, param_grid_lr, cv=StratifiedKFold(5), scoring='roc_auc'
)
grid_lr.fit(X_train, y_train)

# Random Forest Pipeline
pipeline_rf = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', RandomForestClassifier())
])

param_grid_rf = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [None, 10]
}

grid_rf = GridSearchCV(
    pipeline_rf, param_grid_rf, cv=StratifiedKFold(5), scoring='roc_auc'
)
grid_rf.fit(X_train, y_train)

# XGBoost Pipeline
pipeline_xgb = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))
])

param_grid_xgb = {
    'classifier__learning_rate': [0.01, 0.1],
    'classifier__max_depth': [3, 5]
}

grid_xgb = GridSearchCV(
    pipeline_xgb, param_grid_xgb, cv=StratifiedKFold(5), scoring='roc_auc'
)
grid_xgb.fit(X_train, y_train)


#Evaluate Models

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    return {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'roc_auc': roc_auc_score(y_test, y_proba)
    }

models = {
    'Logistic Regression': grid_lr,
    'Random Forest': grid_rf,
    'XGBoost': grid_xgb
}

for name, model in models.items():
    print(f"{name} Performance:")
    print(evaluate_model(model, X_test, y_test))

#Backtesting

# Example with TimeSeriesSplit (if time data were available)
tscv = TimeSeriesSplit(n_splits=5)
X_sorted = X.sort_values('duration')  # Simulate temporal order

for train_index, test_index in tscv.split(X_sorted):
    X_train_ts, X_test_ts = X.iloc[train_index], X.iloc[test_index]
    y_train_ts, y_test_ts = y.iloc[train_index], y.iloc[test_index]
    # Train and evaluate model here (example with XGBoost)
    model = pipeline_xgb.fit(X_train_ts, y_train_ts)
    print(evaluate_model(model, X_test_ts, y_test_ts))

#Save the Best Model

best_model = grid_xgb.best_estimator_
joblib.dump(best_model, 'credit_default_model.pkl')

#Deploy with Flask
#createapi
app = Flask(__name__)
model = joblib.load('credit_default_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    df = pd.DataFrame([data])
    prediction = model.predict(df)
    return jsonify({'default_probability': float(prediction[0])})

if __name__ == '__main__':
    app.run(debug=True)

#Test the API:
curl -X POST -H "Content-Type: application/json" -d '{"checking_account": "A11", "duration": 24, ...}' http://localhost:5000/predict
